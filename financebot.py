# financebot_high_success.py
# æ™ºèƒ½åŒ–é«˜æˆåŠŸç‡è´¢ç»æ–°é—»æŠ“å–ã€AI æ‘˜è¦ï¼ˆDeepseekï¼‰ä¸ ServerChan æ¨é€
# é€‚ç”¨äº GitHub Actions / æœ¬åœ°è¿è¡Œã€‚å¤åˆ¶æ›¿æ¢åŸ financebot.py å³å¯ã€‚

from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import random
import re
from urllib.parse import urlparse

# å¯é€‰åŠ¨æ€æ¸²æŸ“ä¾èµ–ï¼ˆå¦‚æœæœªå®‰è£…ï¼Œè„šæœ¬ä»èƒ½å·¥ä½œä½†æ— æ³•æ¸²æŸ“JSé¡µé¢ï¼‰
try:
    from requests_html import HTMLSession
    RENDER_AVAILABLE = True
except Exception:
    RENDER_AVAILABLE = False

# é…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERVER_CHAN_KEYS_ENV = os.getenv("SERVER_CHAN_KEYS")
if not SERVER_CHAN_KEYS_ENV:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨ GitHub Actions ä¸­è®¾ç½®æ­¤å˜é‡ï¼")
SERVER_CHAN_KEYS = [k.strip() for k in SERVER_CHAN_KEYS_ENV.split(",") if k.strip()]

if not OPENAI_API_KEY:
    raise ValueError("ç¯å¢ƒå˜é‡ OPENAI_API_KEY æœªè®¾ç½®ï¼")

# Deepseek OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com/v1")
DEEPSEEK_MODEL = "deepseek-chat"

# RSSæºï¼ˆæŒ‰éœ€å¢åˆ ï¼‰
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»": {"åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml"},
    "ğŸ’» 36æ°ª": {"36æ°ª": "https://36kr.com/feed"},
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±": "https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
    "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº": "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# åŸŸåå¼ºåˆ¶æ¸²æŸ“ç­–ç•¥ï¼ˆé‡åˆ°è¿™äº›åŸŸåä¼˜å…ˆä½¿ç”¨ renderï¼‰
FORCE_RENDER_DOMAINS = [
    "wallstreetcn.com",
    "36kr.com",
    "bloomberg.com",
    "wsj.com",
    "bbc.com",
]

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# æ™ºèƒ½æŠ“å–æ­£æ–‡å‡½æ•°ï¼ˆé«˜æˆåŠŸç‡ç‰ˆï¼‰
def fetch_article_text(url, retries=3, use_render=True):
    """
    ä¼˜å…ˆä½¿ç”¨ requests + newspaper æå–æ­£æ–‡ã€‚
    å¤±è´¥åå¯é€‰ä½¿ç”¨ requests_html çš„æ¸²æŸ“ä½œä¸º fallbackï¼ˆå¦‚æœå·²å®‰è£…ï¼‰ã€‚
    è¿”å›ï¼šæŠ“å–åˆ°çš„çº¯æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œæˆ–ç‰¹å®šå¤±è´¥å ä½å­—ç¬¦ä¸²ã€‚
    """

    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.1 Safari/537.36",
    ]

    headers = {
        "User-Agent": random.choice(ua_list),
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }

    session = requests.Session()
    session.headers.update(headers)

    domain = urlparse(url).netloc or ""
    # å¦‚æœåŸŸåå‘½ä¸­å¼ºåˆ¶æ¸²æŸ“åˆ—è¡¨ï¼Œå¯ç”¨ render
    if any(d in domain for d in FORCE_RENDER_DOMAINS):
        use_render = True

    for attempt in range(1, retries + 1):
        try:
            print(f"ğŸ“° æŠ“å–ç¬¬ {attempt} æ¬¡: {url}")
            resp = session.get(url, timeout=12, allow_redirects=True)
            if resp.status_code != 200:
                raise Exception(f"HTTP {resp.status_code}")

            # ç®€å•åçˆ¬æ£€æµ‹ï¼šé¡µé¢è¿‡çŸ­æˆ–åŒ…å« anti-bot æ ‡è¯†
            body = resp.text or ""
            if len(body) < 300 and attempt < retries:
                raise Exception(f"é¡µé¢å†…å®¹è¿‡çŸ­ï¼ˆ{len(body)}ï¼‰ï¼Œç–‘ä¼¼åçˆ¬æˆ–é‡å®šå‘å£³")

            # æ£€æµ‹ JS é‡å®šå‘
            if "window.location" in body or "location.href" in body:
                match = re.search(r"location\.href\s*=\s*['\"](.*?)['\"]", body)
                if match:
                    redirected = match.group(1)
                    print(f"ğŸ” æ£€æµ‹åˆ°JSè·³è½¬ï¼Œå°è¯•è·³è½¬è‡³ {redirected}")
                    resp = session.get(redirected, timeout=12, allow_redirects=True)
                    body = resp.text or ""

            # ä½¿ç”¨ newspaper æå–æ­£æ–‡
            article = Article(url)
            article.set_html(body)
            article.parse()
            text = (article.text or "").strip()

            if len(text) > 200:
                # æˆåŠŸ
                print(f"âœ… æŠ“å–æˆåŠŸï¼ˆ{len(text)} å­—ï¼‰")
                return text[:3000]
            else:
                # å¯èƒ½è§£æå¤±è´¥ï¼Œé‡è¯•
                print(f"âš ï¸ æŠ“å–åˆ°æ–‡æœ¬å¤ªçŸ­ï¼ˆ{len(text)} å­—ï¼‰ï¼Œå¯èƒ½å¤±è´¥ï¼Œé‡è¯•...")

        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡å¤±è´¥: {e}")
            time.sleep(2 * attempt)

    # æ¸²æŸ“ fallbackï¼ˆåªåœ¨å¯ç”¨æ—¶å¯ç”¨ï¼‰
    if use_render and RENDER_AVAILABLE:
        try:
            print(f"âš™ï¸ å°è¯•åŠ¨æ€æ¸²æŸ“: {url}")
            session_r = HTMLSession()
            r = session_r.get(url, timeout=20)
            # render å¯èƒ½æ¶ˆè€—æ˜¾è‘—æ—¶é—´ï¼Œè¯·æ ¹æ®éœ€è¦è°ƒæ•´ timeout/sleep
            r.html.render(timeout=30, sleep=2)
            paragraphs = [p.text for p in r.html.find('p') if len(p.text) > 40]
            text = "\n".join(paragraphs)
            if len(text) > 200:
                print(f"âœ… åŠ¨æ€æ¸²æŸ“æˆåŠŸï¼ˆ{len(text)} å­—ï¼‰")
                return text[:3000]
            else:
                print(f"âš ï¸ æ¸²æŸ“åæ­£æ–‡ä»ç„¶è¿‡çŸ­ï¼ˆ{len(text)} å­—ï¼‰")
        except Exception as e:
            print(f"âŒ åŠ¨æ€æ¸²æŸ“å¤±è´¥: {e}")

    print(f"ğŸš« æœ€ç»ˆæŠ“å–å¤±è´¥: {url}")
    return "ï¼ˆæŠ“å–å¤±è´¥ï¼‰"


# æ·»åŠ  User-Agent å¤´è·å– RSS
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)


# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
            else:
                print(f"âš ï¸ RSS è¿”å›ä½†æ—  entries: {url}")
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
        time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None


# è·å–RSSå¹¶çˆ¬æ­£æ–‡ï¼ˆç”¨äºAIåˆ†æï¼‰
def fetch_rss_articles(rss_feeds, max_per_source=5):
    news_data = {}
    analysis_text = ""
    stats = {"total": 0, "success": 0, "failed": 0}

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []
            for entry in feed.entries[:max_per_source]:
                stats['total'] += 1
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', '') or entry.get('guid', '')
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    stats['failed'] += 1
                    continue

                article_text = fetch_article_text(link, retries=3, use_render=False)
                if article_text and not article_text.startswith('ï¼ˆæŠ“å–å¤±è´¥'):
                    stats['success'] += 1
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"
                    print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                else:
                    stats['failed'] += 1
                    print(f"ğŸ”¸ {source} - {title} æŠ“å–å¤±è´¥")

                articles.append(f"- [{title}]({link})")

            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"

        news_data[category] = category_content

    print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡: æ€»è®¡ {stats['total']} ç¯‡ï¼ŒæˆåŠŸ {stats['success']} ç¯‡ï¼Œå¤±è´¥ {stats['failed']} ç¯‡")
    return news_data, analysis_text, stats


# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    completion = openai_client.chat.completionsã€‚create(
        model="deepseek-chat"ï¼Œ
        messages=[
            {"role": "system", "content": """
             ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š
             1. æå–æ–°é—»ä¸­æ¶‰åŠçš„ä¸»è¦è¡Œä¸šå’Œä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šæˆ–ä¸»é¢˜ï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸š/ä¸»é¢˜ã€‚ï¼ˆå¦‚æ–°é—»æœªæä¾›å…·ä½“æ¶¨å¹…ï¼Œè¯·ç»“åˆæè¿°å’Œå¸‚åœºæƒ…ç»ªæ¨æµ‹çƒ­ç‚¹ï¼‰
             2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºï¼š
                - å‚¬åŒ–å‰‚ï¼šåˆ†æè¿‘æœŸä¸Šæ¶¨çš„å¯èƒ½åŸå› ï¼ˆæ”¿ç­–ã€æ•°æ®ã€äº‹ä»¶ã€æƒ…ç»ªç­‰ï¼‰ã€‚
                - å¤ç›˜ï¼šæ¢³ç†è¿‡å»3ä¸ªæœˆè¯¥è¡Œä¸š/ä¸»é¢˜çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®åŠ¨æ€ä¸é˜¶æ®µæ€§èµ°åŠ¿ã€‚
                - å±•æœ›ï¼šåˆ¤æ–­è¯¥çƒ­ç‚¹æ˜¯çŸ­æœŸç‚’ä½œè¿˜æ˜¯æœ‰æŒç»­è¡Œæƒ…æ½œåŠ›ã€‚
             3. å°†ä»¥ä¸Šåˆ†ææ•´åˆä¸ºä¸€ç¯‡1500å­—ä»¥å†…çš„è´¢ç»çƒ­ç‚¹æ‘˜è¦ï¼Œé€»è¾‘æ¸…æ™°ã€é‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…é˜…è¯»ã€‚
             """}ï¼Œ
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key åœ¨ server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 5 ç¯‡æ–‡ç« 
    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=5)
    
    # AIç”Ÿæˆæ‘˜è¦
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\âœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content åœ¨ articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
