# financebot_high_success_fixed.py

from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import random
import re
from urllib.parse import urlparse

# å¯é€‰åŠ¨æ€æ¸²æŸ“ä¾èµ–
try:
    from requests_html import HTMLSession
    RENDER_AVAILABLE = True
except Exception:
    RENDER_AVAILABLE = False

# é…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERVER_CHAN_KEYS_ENV = os.getenv("SERVER_CHAN_KEYS")
if not SERVER_CHAN_KEYS_ENV:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®")
SERVER_CHAN_KEYS = [k.strip() for k in SERVER_CHAN_KEYS_ENV.split(",") if k.strip()]

if not OPENAI_API_KEY:
    raise ValueError("ç¯å¢ƒå˜é‡ OPENAI_API_KEY æœªè®¾ç½®")

# Deepseek OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com/v1")

# RSSæº
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»": {"åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml"},
    "ğŸ’» 36æ°ª": {"36æ°ª": "https://36kr.com/feed"},
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±": "https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
    "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº": "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

FORCE_RENDER_DOMAINS = ["wallstreetcn.com", "36kr.com", "bloomberg.com", "wsj.com", "bbc.com"]

def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

def fetch_article_text(url, retries=3, use_render=True):
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.1 Safari/537.36",
    ]
    headers = {
        "User-Agent": random.choice(ua_list),
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }

    session = requests.Session()
    session.headers.update(headers)
    domain = urlparse(url).netloc or ""
    if any(d in domain for d in FORCE_RENDER_DOMAINS):
        use_render = True

    for attempt in range(1, retries + 1):
        try:
            print(f"ğŸ“° æŠ“å–ç¬¬ {attempt} æ¬¡: {url}")
            resp = session.get(url, timeout=12, allow_redirects=True)
            if resp.status_code != 200:
                raise Exception(f"HTTP {resp.status_code}")
            body = resp.text or ""
            if len(body) < 300 and attempt < retries:
                raise Exception(f"é¡µé¢å†…å®¹è¿‡çŸ­ï¼ˆ{len(body)}ï¼‰")

            if "window.location" in body or "location.href" in body:
                match = re.search(r"location\.href\s*=\s*['\"](.*?)['\"]", body)
                if match:
                    redirected = match.group(1)
                    resp = session.get(redirected, timeout=12)
                    body = resp.text or ""

            article = Article(url)
            article.set_html(body)
            article.parse()
            text = (article.text or "").strip()
            if len(text) > 200:
                return text[:3000]
        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡å¤±è´¥: {e}")
            time.sleep(2 * attempt)

    if use_render and RENDER_AVAILABLE:
        try:
            print(f"âš™ï¸ å°è¯•åŠ¨æ€æ¸²æŸ“: {url}")
            session_r = HTMLSession()
            r = session_r.get(url, timeout=20)
            r.html.render(timeout=30, sleep=2)
            paragraphs = [p.text for p in r.html.find('p') if len(p.text) > 40]
            text = "\n".join(paragraphs)
            if len(text) > 200:
                return text[:3000]
        except Exception as e:
            print(f"âŒ åŠ¨æ€æ¸²æŸ“å¤±è´¥: {e}")

    return "ï¼ˆæŠ“å–å¤±è´¥ï¼‰"

def fetch_feed_with_headers(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    return feedparser.parse(url, request_headers=headers)

def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
        time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}")
    return None

def fetch_rss_articles(rss_feeds, max_per_source=5):
    news_data = {}
    analysis_text = ""
    stats = {"total": 0, "success": 0, "failed": 0}
    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ è·å– {source} RSS: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                continue
            articles = []
            for entry in feed.entries[:max_per_source]:
                stats['total'] += 1
                title = entry.get('title', 'æ— æ ‡é¢˜')
                link = entry.get('link', '') or entry.get('guid', '')
                if not link:
                    stats['failed'] += 1
                    continue
                article_text = fetch_article_text(link)
                if article_text and not article_text.startswith('ï¼ˆæŠ“å–å¤±è´¥'):
                    stats['success'] += 1
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"
                else:
                    stats['failed'] += 1
                articles.append(f"- [{title}]({link})")
            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"
        news_data[category] = category_content
    print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡: æ€» {stats['total']}ï¼ŒæˆåŠŸ {stats['success']}ï¼Œå¤±è´¥ {stats['failed']}")
    return news_data, analysis_text, stats

def summarize(text):
    completion = openai_client.chat.completions.create(
        model=DEEPSEEK_MODEL,
        messages=[
            {"role": "system", "content": """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ç”Ÿæˆ1500å­—ä»¥å†…æ‘˜è¦ï¼š
1. æå–ä¸»è¦è¡Œä¸š/ä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸šã€‚
2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºå‚¬åŒ–å‰‚ã€å¤ç›˜ã€å±•æœ›ã€‚
3. æ‘˜è¦é€»è¾‘æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…ã€‚
"""},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

def send_to_wechat(title, content):
    for key in SERVER_CHAN_KEYS:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")

if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")
    articles_data, analysis_text, stats = fetch_rss_articles(rss_feeds, max_per_source=5)
    summary = summarize(analysis_text)
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
