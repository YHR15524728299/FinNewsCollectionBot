# financebot_high_success.py
# æ™ºèƒ½åŒ–é«˜æˆåŠŸç‡è´¢ç»æ–°é—»æŠ“å–ã€AI æ‘˜è¦ï¼ˆDeepseekï¼‰ä¸ ServerChan æ¨é€
# é€‚ç”¨äº GitHub Actions / æœ¬åœ°è¿è¡Œã€‚å¤åˆ¶æ›¿æ¢åŸ financebot.py å³å¯ã€‚

from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import random
import re
from urllib.parse import urlparse

# å¯é€‰åŠ¨æ€æ¸²æŸ“ä¾èµ–ï¼ˆå¦‚æœæœªå®‰è£…ï¼Œè„šæœ¬ä»èƒ½å·¥ä½œä½†æ— æ³•æ¸²æŸ“JSé¡µé¢ï¼‰
try:
    from requests_html import HTMLSession
    RENDER_AVAILABLE = True
except Exception:
    RENDER_AVAILABLE = False

# é…ç½®ï¼ˆç¯å¢ƒå˜é‡ï¼‰
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERVER_CHAN_KEYS_ENV = os.getenv("SERVER_CHAN_KEYS")
if not SERVER_CHAN_KEYS_ENV:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨ GitHub Actions ä¸­è®¾ç½®æ­¤å˜é‡ï¼")
SERVER_CHAN_KEYS = [k.strip() for k in SERVER_CHAN_KEYS_ENV.split(",") if k.strip()]

if not OPENAI_API_KEY:
    raise ValueError("ç¯å¢ƒå˜é‡ OPENAI_API_KEY æœªè®¾ç½®ï¼")

# Deepseek OpenAI client
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com/v1")
DEEPSEEK_MODEL = "deepseek-chat"

# RSSæºï¼ˆæŒ‰éœ€å¢åˆ ï¼‰
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»": {"åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml"},
    "ğŸ’» 36æ°ª": {"36æ°ª": "https://36kr.com/feed"},
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±": "https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
    "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº": "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# åŸŸåå¼ºåˆ¶æ¸²æŸ“ç­–ç•¥ï¼ˆé‡åˆ°è¿™äº›åŸŸåä¼˜å…ˆä½¿ç”¨ renderï¼‰
FORCE_RENDER_DOMAINS = [
    "wallstreetcn.com",
    "36kr.com",
    "bloomberg.com",
    "wsj.com",
    "bbc.com",
]

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# æ™ºèƒ½æŠ“å–æ­£æ–‡å‡½æ•°ï¼ˆé«˜æˆåŠŸç‡ç‰ˆï¼‰
def fetch_article_text(url, retries=3, use_render=True):
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.1 Safari/537.36",
    ]

    headers = {
        "User-Agent": random.choice(ua_list),
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }

    session = requests.Session()
    session.headers.update(headers)

    domain = urlparse(url).netloc or ""
    if any(d in domain for d in FORCE_RENDER_DOMAINS):
        use_render = True

    for attempt in range(1, retries + 1):
        try:
            print(f"ğŸ“° æŠ“å–ç¬¬ {attempt} æ¬¡: {url}")
            resp = session.get(url, timeout=12, allow_redirects=True)
            if resp.status_code != 200:
                raise Exception(f"HTTP {resp.status_code}")

            body = resp.text or ""
            if len(body) < 300 and attempt < retries:
                raise Exception(f"é¡µé¢å†…å®¹è¿‡çŸ­ï¼ˆ{len(body)}ï¼‰ï¼Œç–‘ä¼¼åçˆ¬æˆ–é‡å®šå‘å£³")

            if "window.location" in body or "location.href" in body:
                match = re.search(r"location\.href\s*=\s*['\"](.*?)['\"]", body)
                if match:
                    redirected = match.group(1)
                    print(f"ğŸ” æ£€æµ‹åˆ°JSè·³è½¬ï¼Œå°è¯•è·³è½¬è‡³ {redirected}")
                    resp = session.get(redirected, timeout=12, allow_redirects=True)
                    body = resp.text or ""

            article = Article(url)
            article.set_html(body)
            article.parse()
            text = (article.text or "").strip()

            if len(text) > 200:
                print(f"âœ… æŠ“å–æˆåŠŸï¼ˆ{len(text)} å­—ï¼‰")
                return text[:3000]
            else:
                print(f"âš ï¸ æŠ“å–åˆ°æ–‡æœ¬å¤ªçŸ­ï¼ˆ{len(text)} å­—ï¼‰ï¼Œå¯èƒ½å¤±è´¥ï¼Œé‡è¯•...")

        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡å¤±è´¥: {e}")
            time.sleep(2 * attempt)

    if use_render and RENDER_AVAILABLE:
        try:
            print(f"âš™ï¸ å°è¯•åŠ¨æ€æ¸²æŸ“: {url}")
            session_r = HTMLSession()
            r = session_r.get(url, timeout=20)
            r.html.render(timeout=30, sleep=2)
            paragraphs = [p.text for p in r.html.find('p') if len(p.text) > 40]
            text = "\n".join(paragraphs)
            if len(text) > 200:
                print(f"âœ… åŠ¨æ€æ¸²æŸ“æˆåŠŸï¼ˆ{len(text)} å­—ï¼‰")
                return text[:3000]
            else:
                print(f"âš ï¸ æ¸²æŸ“åæ­£æ–‡ä»ç„¶è¿‡çŸ­ï¼ˆ{len(text)} å­—ï¼‰")
        except Exception as e:
            print(f"âŒ åŠ¨æ€æ¸²æŸ“å¤±è´¥: {e}")

    print(f"ğŸš« æœ€ç»ˆæŠ“å–å¤±è´¥: {url}")
    return "ï¼ˆæŠ“å–å¤±è´¥ï¼‰"


def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)


def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
            else:
                print(f"âš ï¸ RSS è¿”å›ä½†æ—  entries: {url}")
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
        time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None



def fetch_rss_articles(rss_feeds, max_per_source=5):
    news_data = {}
    analysis_text = ""
    stats = {"total": 0, "success": 0, "failed": 0}

    for category, sources åœ¨ rss_feeds.items():
        category_content = ""
        for source, url åœ¨ sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []
            for entry åœ¨ feed.entries[:max_per_source]:
                stats['total'] += 1
                title = entry.get('title'ï¼Œ 'æ— æ ‡é¢˜')
                link = entry.get('link'ï¼Œ '') or entry.get('guid', '')
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    stats['failed'] += 1
                    text = fetch_article_text(link)
                if text == "ï¼ˆæŠ“å–å¤±è´¥ï¼‰":
                    stats['failed'] += 1
                else:
                    stats['success'] += 1

                article_summary = f"ã€{source}ã€‘{title}\n{link}\n{text}\n\n"
                category_content += article_summary
                analysis_text += article_summary

        if category_content:
            news_data[category] = category_content

    print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡: æ€» {stats['total']}ï¼ŒæˆåŠŸ {stats['success']}ï¼Œå¤±è´¥ {stats['failed']}")
    return news_data, analysis_text


def summarize(text):
    completion = openai_client.chat.completions.create(
        model=DEEPSEEK_MODEL,
        messages=[
            {"role": "system", "content": """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ç”Ÿæˆ1500å­—ä»¥å†…æ‘˜è¦ï¼š
1. æå–ä¸»è¦è¡Œä¸š/ä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸šã€‚
2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºå‚¬åŒ–å‰‚ã€å¤ç›˜ã€å±•æœ›ã€‚
3. æ‘˜è¦é€»è¾‘æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…ã€‚
"""},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

def send_to_wechat(title, content):
    for key in SERVER_CHAN_KEYS:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")

if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")
    articles_data, analysis_text, stats = fetch_rss_articles(rss_feeds, max_per_source=5)
    summary = summarize(analysis_text)
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
