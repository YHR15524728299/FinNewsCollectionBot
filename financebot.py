# financebot_final.py
# âœ… æœ€ç»ˆç‰ˆï¼šDeepSeek æ‘˜è¦ + RSS æŠ“å– + ServerChan æ¨é€ï¼ˆé™åˆ¶é“¾æ¥â‰¤30æ¡ï¼‰

from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
import random
import re
from urllib.parse import urlparse

# =============================
# ç¯å¢ƒé…ç½®
# =============================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERVER_CHAN_KEYS_ENV = os.getenv("SERVER_CHAN_KEYS")

if not OPENAI_API_KEY:
    raise ValueError("âŒ ç¯å¢ƒå˜é‡ OPENAI_API_KEY æœªè®¾ç½®ï¼")
if not SERVER_CHAN_KEYS_ENV:
    raise ValueError("âŒ ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼")

SERVER_CHAN_KEYS = [k.strip() for k in SERVER_CHAN_KEYS_ENV.split(",") if k.strip()]

# DeepSeek Client
openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com/v1")
DEEPSEEK_MODEL = "deepseek-chat"

# =============================
# RSS æºï¼ˆä¿æŒä¸æ”¹åŠ¨ï¼‰
# =============================
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»": {"åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml"},
    "ğŸ’» 36æ°ª": {"36æ°ª": "https://36kr.com/feed"},
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±": "https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
    "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº": "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}

# =============================
# å·¥å…·å‡½æ•°
# =============================
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d")

def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)

def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è·å– {url} å¤±è´¥: {e}")
        time.sleep(delay)
    return None

# =============================
# æŠ“å– RSS å¹¶æå–é“¾æ¥
# =============================
def fetch_rss_articles(rss_feeds, max_per_source=5):
    links = []
    for category, sources in rss_feeds.items():
        for source, url in sources.items():
            feed = fetch_feed_with_retry(url)
            if not feed:
                continue
            for entry in feed.entries[:max_per_source]:
                link = entry.get("link", "")
                if link and link.startswith("http"):
                    links.append(link)
    return links

# =============================
# DeepSeek æ‘˜è¦ï¼ˆä¿æŒä½ åŸç‰ˆä¸æ”¹ï¼‰
# =============================
def summarize(text):
    completion = openai_client.chat.completions.create(
        model=DEEPSEEK_MODEL,
        messages=[
            {"role": "system", "content": """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ç”Ÿæˆ1500å­—ä»¥å†…æ‘˜è¦ï¼š
1. æå–ä¸»è¦è¡Œä¸š/ä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸šã€‚
2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºå‚¬åŒ–å‰‚ã€å¤ç›˜ã€å±•æœ›ã€‚
3. æ‘˜è¦é€»è¾‘æ¸…æ™°ï¼Œé‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…ã€‚
"""},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# =============================
# ServerChan æ¨é€
# =============================
def send_to_wechat(title, content):
    for key in SERVER_CHAN_KEYS:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        resp = requests.post(url, data=data, timeout=10)
        if resp.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, {resp.text}")

# =============================
# ä¸»æµç¨‹
# =============================
if __name__ == "__main__":
    today_str = today_date()
    print("ğŸš€ æ­£åœ¨æŠ“å– RSS æ–°é—» ...")
    links = fetch_rss_articles(rss_feeds)
    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…± {len(links)} æ¡")

    # é™åˆ¶æœ€å¤š 30 æ¡é“¾æ¥
    links = links[:30]
    joined_links = "\n".join([f"- {url}" for url in links])

    print("ğŸ§  æ­£åœ¨ç”Ÿæˆ DeepSeek æ‘˜è¦ ...")
    summary = summarize("\n".join(links))

    final_summary = f"""ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**

âœï¸ **AI æ‘˜è¦ï¼š**
{summary}

---

ğŸ“ **æ–°é—»é“¾æ¥ï¼ˆå…±{len(links)}æ¡ï¼‰ï¼š**
{joined_links}
"""

    # æ§åˆ¶æ–¹ç³–æ¨é€å­—æ•°ï¼ˆçº¦ 2000 å­—å®‰å…¨ï¼‰
    if len(final_summary) > 2000:
        final_summary = final_summary[:1900] + "\n\n...ï¼ˆå†…å®¹è¿‡é•¿ï¼Œéƒ¨åˆ†å·²çœç•¥ï¼‰"

    print("ğŸ“¤ æ­£åœ¨æ¨é€è‡³ ServerChan ...")
    send_to_wechat(f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", final_summary)
    print("âœ… å®Œæˆï¼")
